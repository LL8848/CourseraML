{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4 Neural Networks Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Neural Neworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random #To pick random images to display\n",
    "from scipy.special import expit #Vectorized sigmoid function\n",
    "from scipy.optimize import minimize # for minimizing the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'data/ex3data1.mat'\n",
    "mat = scipy.io.loadmat( datafile )\n",
    "X, y = mat['X'], mat['y']\n",
    "[m,n] = X.shape  \n",
    "y = y.ravel() # turn y into a 1d array\n",
    "# y[y == 10] = 0 # in the Coursara's course, \n",
    "               # character '0' was labeled as '10' for the convenience in MATLAB\n",
    "               # here, I change it back to '0' for the convenience in Python\n",
    "# convert each character label to a vector\n",
    "Y = np.zeros((m,10))\n",
    "for yi,Yi in zip(y,Y):\n",
    "    Yi[yi-1] = 1.0\n",
    "        \n",
    "#Insert a column of 1's to X as usual\n",
    "X = np.insert(X,0,1,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def getDatumImg(row):\n",
    "    \"\"\"\n",
    "    Function that is handed a single np array with shape 1x400,\n",
    "    crates an image object from it, and returns it\n",
    "    \"\"\"\n",
    "    width, height = 20, 20\n",
    "    square = row[1:].reshape(width,height)\n",
    "    return square.T\n",
    "    \n",
    "def displayData(indices_to_display = None):\n",
    "    \"\"\"\n",
    "    Function that picks 100 random rows from X, creates a 20x20 image from each,\n",
    "    then stitches them together into a 10x10 grid of images, and shows it.\n",
    "    \"\"\"\n",
    "    width, height = 20, 20\n",
    "    nrows, ncols = 10, 10\n",
    "    if not indices_to_display:\n",
    "        indices_to_display = random.sample(range(X.shape[0]), nrows*ncols)\n",
    "        \n",
    "    big_picture = np.zeros((height*nrows,width*ncols))\n",
    "    \n",
    "    irow, icol = 0, 0\n",
    "    for idx in indices_to_display:\n",
    "        if icol == ncols:\n",
    "            irow += 1\n",
    "            icol  = 0\n",
    "        iimg = getDatumImg(X[idx])\n",
    "        big_picture[irow*height:irow*height+iimg.shape[0],icol*width:icol*width+iimg.shape[1]] = iimg\n",
    "        icol += 1\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "#     img = scipy.misc.toimage( big_picture )\n",
    "    img = Image.fromarray(big_picture)\n",
    "    plt.imshow(img,cmap = cm.Greys_r)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    \n",
    "def display1sample(X,y,nrow):\n",
    "    \"\"\"\n",
    "    Display a single sample (row)\n",
    "    \"\"\"\n",
    "    img = Image.fromarray(getDatumImg(X[nrow]))\n",
    "    plt.imshow(img,cmap = cm.Greys_r)\n",
    "    print(f'Labeled character is {y[nrow]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled character is 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the display functions\n",
    "display1sample(X,y,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmat = scipy.io.loadmat('data/ex4weights1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = weightmat['Theta1'] # 25 by 401\n",
    "Theta2 = weightmat['Theta2'] # 10 by 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Feedforward the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(theta1,theta2,X):\n",
    "    \n",
    "    \"\"\"\n",
    "        theta1,2: 2d array\n",
    "        X: 2d of m by n\n",
    "    \"\"\"\n",
    "    z2 = theta1 @ X.T # z2: 25 by m\n",
    "    a2 = expit(z2) \n",
    "    a2 = np.insert(a2,0,1,axis=0) # a2: 26 by m\n",
    "    z3 = theta2 @ a2 # z3: 10 by m\n",
    "    a3 = expit(z3)\n",
    "    \n",
    "    return a3.T # return a 2d array of m by 10\n",
    "\n",
    "\n",
    "def h(theta1,theta2,x):\n",
    "    \n",
    "    \"\"\"\n",
    "        theta1,2: 2d array\n",
    "        x: 1d of n\n",
    "    \"\"\"    \n",
    "    z2 = theta1 @ x\n",
    "    a2 = expit(z2)\n",
    "    a2 = np.insert(a2,0,1)\n",
    "    z3 = theta2 @ a2\n",
    "    a3 = expit(z3)\n",
    "    \n",
    "    return a3 # 1d array of 10\n",
    "\n",
    "def forwardProg(thetalist,x):\n",
    "    \n",
    "    \"\"\"\n",
    "        thetalist: list of theta\n",
    "    \"\"\"\n",
    "    a = []\n",
    "    z = []\n",
    "    a.append(x[1:]) # input layer\n",
    "    for theta in thetalist:\n",
    "        # propagate to next layer\n",
    "        z.append(theta @ np.insert(a[-1],0,1))\n",
    "        a.append(expit(z[-1]))\n",
    "    \n",
    "    return a[1:], z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.79026796e-04, 2.41495958e-03, 3.44755685e-03, 4.05616281e-05,\n",
       "       6.53412433e-03, 1.75930169e-03, 1.15788527e-02, 2.39107046e-03,\n",
       "       1.97025086e-03, 9.95696931e-01])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(Theta1,Theta2,X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "al, zl = forwardProg([Theta1,Theta2],X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(thetalist,X,Y):\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute cost function without regularization\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    # unvectorize\n",
    "    mysum = 0\n",
    "    for i in range(0,m):\n",
    "        a, z = forwardProg(thetalist,X[i])\n",
    "        term1 =  (- Y[i]) @ np.log(a[-1]) # a[-1] is last element of a, which is the output layer\n",
    "        term2 = (1 - Y[i]) @ np.log(1 - a[-1])\n",
    "        mysum += (term1 - term2)\n",
    "\n",
    "    J = mysum / m    \n",
    "    \n",
    "    return J\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgcostFunction(thetalist,X,Y,mylambda = 0):\n",
    "\n",
    "    \"\"\"\n",
    "        Compute cost function with regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    J0 = costFunction(thetalist,X,Y)\n",
    "        \n",
    "    rgterm = 0\n",
    "    for theta in thetalist:\n",
    "        rgterm += np.sum(np.square(theta[:,1:]))\n",
    "    rgterm = rgterm * mylambda / (2 * X.shape[0])\n",
    "    \n",
    "    J = J0 + rgterm\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2876291651613188"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costFunction([Theta1,Theta2],X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38376985909092354"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgcostFunction([Theta1,Theta2],X,Y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sigmoid gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid gradient\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    g = expit(z)\n",
    "    grad = g * (1 - g)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5395807735907655e-05"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoidGradient(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some global variables I'm suing to ensure the sizes\n",
    "# of various matrices are correct\n",
    "#these are NOT including bias nits\n",
    "# input_layer_size = 400\n",
    "# hidden_layer_size = 25\n",
    "# output_layer_size = 10\n",
    "\n",
    "def randomInitialization(lsizelst,eps_ini = 0.12):\n",
    "    \"\"\"\n",
    "    lsizelst is a list of which each elemeter is the size of each layer (excluding the bias unit)\n",
    "    \"\"\"\n",
    "    \n",
    "    [input_layer_size,hidden_layer_size,output_layer_size] = lsizelst \n",
    "\n",
    "    theta1 = np.random.random((hidden_layer_size,input_layer_size + 1 )) * 2 * eps_ini - eps_ini\n",
    "    theta2 = np.random.random((output_layer_size,hidden_layer_size + 1 )) * 2 * eps_ini - eps_ini\n",
    "    \n",
    "    return [theta1,theta2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "aa = randomInitialization([400,25,10])\n",
    "print(aa[0].shape,aa[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[]: length of 2\n",
    "# z[]: length of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGradient(thetalist,X,Y):\n",
    "    \n",
    "    [theta1, theta2] = thetalist\n",
    "    DELTA1 = np.zeros_like(theta1)\n",
    "    DELTA2 = np.zeros_like(theta2)\n",
    "    for i in range(X.shape[0]):\n",
    "        alist, zlist = forwardProg(thetalist,X[i])   # alist, zlist are lists with length of 2\n",
    "                                             # Here, a = [a2, a3],  z = [z2, z3]\n",
    "\n",
    "        [a2,a3] = alist\n",
    "        [z2,z3] = zlist\n",
    "        a2 = np.insert(a2,0,1) # add the bias term ( +1 ) to the activation vectors\n",
    "        a1 = X[i] # here X[i] already includes bias unit\n",
    "        z2 = np.insert(z2,0,1)\n",
    "        delta3 = a3 - Y[i] # 1d array of 10\n",
    "        delta2 = (theta2.T @ delta3) * sigmoidGradient(z2)\n",
    "        delta2 = delta2[1:]\n",
    "\n",
    "        DELTA1 += delta2.reshape((-1,1)) @ a1.reshape((1,-1))\n",
    "        DELTA2 += delta3.reshape((-1,1)) @ a2.reshape((1,-1))\n",
    "       \n",
    "    return DELTA1 / X.shape[0], DELTA2 / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad1, grad2 = gradient([theta1,theta2],X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll(matrixlst):\n",
    "    \"\"\"\n",
    "    unroll a matrix list (eg, theta list) to a long vector\n",
    "    \"\"\"\n",
    "    length = 0\n",
    "    vector = np.array([])\n",
    "    for matrix in matrixlst:\n",
    "        vector = np.concatenate((vector,matrix.ravel()))\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def roll(vector,lsizelst):\n",
    "    \"\"\"\n",
    "    lsizelst is a list of which each elemeter is the size of each layer (excluding the bias unit)\n",
    "    \"\"\"\n",
    "    ntheta = len(lsizelst) - 1\n",
    "    matrixlst = []\n",
    "#     for size1,size0 in zip(layersizelist[1:],layersizelist[:-1]):\n",
    "    index = 0\n",
    "    for i in range(ntheta):\n",
    "        nrow = lsizelst[i+1]\n",
    "        ncol = lsizelst[i]+1\n",
    "        matrixlst.append(vector[index:index+nrow*ncol].reshape((nrow,ncol)))\n",
    "#         print(matrixlst[i].shape)\n",
    "        index += nrow*ncol\n",
    "    return matrixlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = unroll([Theta1,Theta2])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = roll(a,[400,25,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNumericalGradient(thetalist,X,Y,lsizelst):\n",
    "    eps = 0.0001   \n",
    "    vtheta = unroll(thetalist) # unroll the theta matrices to a long vector\n",
    "    vnumgrad = np.zeros_like(vtheta) # vector of numerical gradient\n",
    "    for i in range(vnumgrad.size):\n",
    "        vthetaPlus = np.copy(vtheta)\n",
    "        vthetaMinus = np.copy(vtheta)\n",
    "        vthetaPlus[i] += eps\n",
    "        vthetaMinus[i] -= eps\n",
    "        JthetaPlus = costFunction(roll(vthetaPlus,lsizelst),X,Y) \n",
    "        JthetaMinus = costFunction(roll(vthetaMinus,lsizelst),X,Y)\n",
    "        vnumgrad[i] = (JthetaPlus - JthetaMinus) / (2 * eps)\n",
    "\n",
    "    return roll(vnumgrad,lsizelst)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNNGradients(X,Y,ninput = 20,mtest = 20,nhidden = 15,ti = 0):\n",
    "    \"\"\"\n",
    "    Use a fraction of input units (ie less features) and less hidden units to check the Gradient\n",
    "    ntest: number of features (or input layer units, not including the bias unit)\n",
    "    mtest: number of samples\n",
    "    nhidden: number of hidden units\n",
    "    ti: index of the first sample (better randomly choose one)\n",
    "    \"\"\"\n",
    "    print(\"Checking Neural Networks gradients ...\")\n",
    "    print(f\"Number of test input units: {ninput}\")\n",
    "    print(f\"Number of test hidden units: {nhidden}\")\n",
    "    print(f\"Use {mtest} samples, from {ti} to {ti + mtest - 1}\")\n",
    "    # starting index, better \n",
    "    Xtest = X[ti : ti + mtest,0 : ninput + 1]\n",
    "    Ytest = Y[ti : ti + mtest]\n",
    "\n",
    "    layer_size_list = [ninput,nhidden,10]\n",
    "    [th1,th2] = randomInitialization(layer_size_list)\n",
    "    [g1,g2] = computeGradient([th1,th2],Xtest,Ytest)\n",
    "    [ng1,ng2] = computeNumericalGradient([th1,th2],Xtest,Ytest,layer_size_list)\n",
    "    \n",
    "    difference = np.abs(unroll([g1,g2]) - unroll([ng1,ng2])) # a long unrolled vector\n",
    "    avediff = np.sum(difference) / difference.size # average difference between numerical gradient and backprog gradient\n",
    "    maxdiff = np.max(difference)\n",
    "    \n",
    "    print(\"\\nCheck Completed.\")\n",
    "    print(f\"Average difference: {avediff}\")\n",
    "    print(f\"Maximum difference: {maxdiff}\")\n",
    "    \n",
    "    return avediff, maxdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Neural Networks gradients ...\n",
      "Number of test input units: 20\n",
      "Number of test hidden units: 15\n",
      "Use 60 samples, from 103 to 162\n",
      "\n",
      "Check Completed.\n",
      "Average difference: 1.391867642249423e-11\n",
      "Maximum difference: 1.1661921428540722e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.391867642249423e-11, 1.1661921428540722e-10)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkNNGradients(X,Y,ninput = 20,mtest = 60,ti = 103)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Regularized Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgcomputeGradient(thetalist,X,Y,mylambda):\n",
    "    \n",
    "    gradlist = computeGradient(thetalist,X,Y)\n",
    "    rggradlist = []\n",
    "    for grad,theta in zip(gradlist,thetalist):\n",
    "        rgterm = np.insert((mylambda / X.shape[0]) * theta[:,1:],0,0,axis=1)\n",
    "        rggradlist.append(grad + rgterm)\n",
    "        \n",
    "    return rggradlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgcomputeNumericalGradient(thetalist,X,Y,lsizelst,mylambda = 0):\n",
    "    eps = 0.0001   \n",
    "    vtheta = unroll(thetalist) # unroll the theta matrices to a long vector\n",
    "    vnumgrad = np.zeros_like(vtheta) # vector of numerical gradient\n",
    "    for i in range(vnumgrad.size):\n",
    "        vthetaPlus = np.copy(vtheta)\n",
    "        vthetaMinus = np.copy(vtheta)\n",
    "        vthetaPlus[i] += eps\n",
    "        vthetaMinus[i] -= eps\n",
    "        JthetaPlus = rgcostFunction(roll(vthetaPlus,lsizelst),X,Y,mylambda) \n",
    "        JthetaMinus = rgcostFunction(roll(vthetaMinus,lsizelst),X,Y,mylambda)\n",
    "        vnumgrad[i] = (JthetaPlus - JthetaMinus) / (2 * eps)\n",
    "\n",
    "    return roll(vnumgrad,lsizelst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgcheckNNGradients(X,Y,mylambda = 0,ninput = 20,mtest = 20,nhidden = 15,ti = 0):\n",
    "    \"\"\"\n",
    "    Use a fraction of input units (ie less features) and less hidden units to check the Gradient\n",
    "    ntest: number of features (or input layer units, not including the bias unit)\n",
    "    mtest: number of samples\n",
    "    nhidden: number of hidden units\n",
    "    ti: index of the first sample (better randomly choose one)\n",
    "    \"\"\"\n",
    "    print(\"Checking Neural Networks gradients ...\")\n",
    "    print(f\"Number of test input units: {ninput}\")\n",
    "    print(f\"Number of test hidden units: {nhidden}\")\n",
    "    print(f\"Use {mtest} samples, from {ti} to {ti + mtest - 1}\")\n",
    "    # starting index, better \n",
    "    Xtest = X[ti : ti + mtest,0 : ninput + 1]\n",
    "    Ytest = Y[ti : ti + mtest]\n",
    "\n",
    "    layer_size_list = [ninput,nhidden,10]\n",
    "    [th1,th2] = randomInitialization(layer_size_list)\n",
    "    [g1,g2] = rgcomputeGradient([th1,th2],Xtest,Ytest,mylambda)\n",
    "    [ng1,ng2] = rgcomputeNumericalGradient([th1,th2],Xtest,Ytest,layer_size_list,mylambda)\n",
    "    \n",
    "    difference = np.abs(unroll([g1,g2]) - unroll([ng1,ng2])) # a long unrolled vector\n",
    "    avediff = np.sum(difference) / difference.size # average difference between numerical gradient and backprog gradient\n",
    "    maxdiff = np.max(difference)\n",
    "    \n",
    "    print(\"\\nCheck Completed.\")\n",
    "    print(f\"Average difference: {avediff}\")\n",
    "    print(f\"Maximum difference: {maxdiff}\")\n",
    "    \n",
    "    return avediff, maxdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Neural Networks gradients ...\n",
      "Number of test input units: 30\n",
      "Number of test hidden units: 15\n",
      "Use 20 samples, from 153 to 172\n",
      "\n",
      "Check Completed.\n",
      "Average difference: 4.024898492578232e-12\n",
      "Maximum difference: 6.234229799062518e-11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.024898492578232e-12, 6.234229799062518e-11)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgcheckNNGradients(X,Y,mylambda=0.1,ninput=30,mtest=20,ti=153)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def URrgcomputeGradient(vthetalist,X,Y,lsizelst,mylambda = 0):\n",
    "    \"\"\"\n",
    "    UnRolled version of computeGradient,using a unrolled vector of thetalist as input\n",
    "    \"\"\"\n",
    "    thetalist = roll(vthetalist,lsizelst)\n",
    "    gradlist = rgcomputeGradient(thetalist,X,Y,mylambda)\n",
    "    \n",
    "    return unroll(gradlist)\n",
    "    \n",
    "\n",
    "def URrgcostFunction(vthetalist,X,Y,lsizelst,mylambda = 0):\n",
    "    thetalist = roll(vthetalist,lsizelst)\n",
    "    return rgcostFunction(thetalist,X,Y,mylambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniCost(theta0,X,Y,lsizelst,mylambda = .0,maxiter = 50):\n",
    "    \"\"\"\n",
    "        minimizing the cost function\n",
    "        theta0 must be a 1d array (obtained by unrolling the theta list)\n",
    "    \"\"\"\n",
    "    \n",
    "    result = minimize(URrgcostFunction, x0=theta0, args=(X,Y,lsizelst,mylambda), method='CG',\n",
    "                      jac = URrgcomputeGradient, options={'maxiter':maxiter,'disp':True})\n",
    "\n",
    "    return roll(result.x,lsizelst) # return the theta (1d array of n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def LearnNN(X,Y,mylambda,layer_size_list = [400,25,10],maxiter = 50):\n",
    "    print(\"Learning Neural Network parameters\")\n",
    "    print(f\"lambda = {mylambda}, max iteration = {maxiter}\")\n",
    "    layer_size_list = [400,25,10]\n",
    "    mylambda = 0.1\n",
    "    theta0_lst = randomInitialization(layerSize)\n",
    "#     print(f\"theta1 shape: {theta0_lst[0].shape},theta2 shape: {theta0_lst[1].shape}\" )\n",
    "    vtheta0 = unroll(theta0_lst) # unroll the theta list to a long 1d array\n",
    "    theta_mini = miniCost(vtheta0,X,Y,layer_size_list,mylambda,maxiter)\n",
    "    print(\"Completed.\")\n",
    "    return theta_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Neural Network parameters\n",
      "lambda = 0.1, max iteration = 400\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.071358\n",
      "         Iterations: 400\n",
      "         Function evaluations: 1135\n",
      "         Gradient evaluations: 1135\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "THETA1 = LearnNN(X,Y,mylambda=0.1,maxiter=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Neural Network parameters\n",
      "lambda = 1, max iteration = 400\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.072896\n",
      "         Iterations: 400\n",
      "         Function evaluations: 1156\n",
      "         Gradient evaluations: 1156\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "THETA2 = LearnNN(X,Y,mylambda=1,maxiter=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Neural Network parameters\n",
      "lambda = 0, max iteration = 400\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.068683\n",
      "         Iterations: 400\n",
      "         Function evaluations: 1164\n",
      "         Gradient evaluations: 1164\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "THETA3 = LearnNN(X,Y,mylambda=0,maxiter=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNN(theta1,theta2,x):\n",
    "    \n",
    "    \"\"\"\n",
    "        use trained neural network parameters to predict (i.e. forward propagation)\n",
    "    \"\"\"\n",
    "    z2 = theta1 @ x\n",
    "    a2 = expit(z2) # here, a2 is a 1d array of 25\n",
    "    a2 = np.insert(a2,0,1) # insert the bias unit\n",
    "    \n",
    "    z3 = theta2 @ a2\n",
    "    a3 = expit(z3)\n",
    "    \n",
    "#     print(a3)\n",
    "    prey = np.argmax(a3)+1\n",
    "#     if prey == 10:\n",
    "#         prey = 0\n",
    "    return prey\n",
    "\n",
    "# note: \"0\" has been labeled as 10 here, due to the MATLAB convenience in the homework\n",
    "def ExamineNN(thetalist,X,y):\n",
    "    [theta1, theta2] = thetalist\n",
    "    ncorrect = 0\n",
    "    nwrong = 0\n",
    "    for xrow,yrow in zip(X,y):\n",
    "        prey = predictNN(theta1,theta2,xrow)\n",
    "        if prey == yrow:\n",
    "            ncorrect += 1\n",
    "        else:\n",
    "            nwrong +=1\n",
    "    print(f\"accuracy = {ncorrect / y.size * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 100.00 %\n"
     ]
    }
   ],
   "source": [
    "ExamineNN(THETA3,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkresult(X,y,thetalist,nrow):\n",
    "    img = Image.fromarray(getDatumImg(X[nrow]))\n",
    "    plt.imshow(img,cmap = cm.Greys_r)\n",
    "    prey = predictNN(thetalist[0],thetalist[1],X[nrow])\n",
    "    print(prey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkresult(X,y,THETA3,39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
